{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a short tutorial on NLP, task is to create a model to predict whether given email is a spam or ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Steps to create the model\n",
    "    1 Load the dataset (dataset used in this tutorial is available on https://archive.ics.uci.edu/ml/datasets.php)\n",
    "    \n",
    "    2. Write a function, which would iterate to the whole dataset and will perform following tasks:\n",
    "        2.1 it will first remove all the special characters from the message\n",
    "        2.2 will covernt all the messages to lower case messages\n",
    "        2.3 split the message and make a list of words, iterate each word to remove stopwords & get its stema/lemma (more \n",
    "            on this later)\n",
    "        2,4 create the message back from the new set of words that we got after removing stopwords and getting stema/lemma\n",
    "        2.5 append the clean message to message_set\n",
    "        \n",
    "    3. Create bag of words\n",
    "    \n",
    "    4. Create two models using MultinomialNB(), one model for the message set which was created using Stemming & one for Lemmatization\n",
    "    \n",
    "    5. Evaluate the models and check their accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the necessary liabraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                            Message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets load the dataset, the dataset is availabe on the website \"https://archive.ics.uci.edu/ml/datasets.php\"\n",
    "\n",
    "# file has labels(spam/ham) & the message itself, labels and messages are tab seperated, hence we are goint to use \"\\t\" as the \n",
    "# seperator, dataset does not have any column names, so, lets name them as \"Label\" & \"Message\"\n",
    "\n",
    "spamham = pd.read_csv(\"SMSSpamCollection.txt\",sep = \"\\t\", names = [\"Label\",\"Message\"])\n",
    "spamham.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the no. of rows and columns present in the dataset\n",
    "spamham.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Removing noise from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Noise?\n",
    "Take a look at the messages present in the dataset, you would find words like \"Go\" & \"go\". Words like \"is\", \"the\", words like \"drive\" \"driving\". This is basically noise and need to be cleaned as explained below.\n",
    "\n",
    "### StopWords\n",
    "Words like \"The\" \"is\" etc are the most frequent words in any email/text, we are going to remove these words from the messages  present in the dataset before training the model as these words do not add any value to model's predictive power.\n",
    "\n",
    "### Stemming\n",
    "It’s a rule-based technique that just chops off the suffix of a word to get its root form which is called the ‘stem’. For example, \"driver\" would be converted to \"driv\" and \"going\" would be converted to \"go\", obviously \"driv\" is not a valid word, so we need better options than Stemming.\n",
    "\n",
    "### Lemmatization: \n",
    "Lemmatization is more accurate than Stemming, This method takes an input word and searches for its base word by going recursively through all the variations of dictionary words. hence, driver would be converted to \"drvie\" whereas \"going\" would be converted to \"go\". This method is slower than Stemming though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following two Classes are needed to convert the words to their stem and lemma forms.\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function to preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets clean the dataset, by removing Special characters.\n",
    "\n",
    "# re.sub used below tries to look for each occurrence of the pattern in the string and replaces all of them by the replacement \n",
    "#string, returns the same original if match not found. \n",
    "\n",
    "#I have used Meta-sequence \\w as first parameter to re.sub, which is used to match all the alphanumeric characters.\n",
    "# \"^\" in front of \"\\w\" would actually negate this condition, and hence, \"^\\w\" would end up retaining only alphanumeric characters\n",
    "# present in the messages.\n",
    "\n",
    "def preprocess(data,stem = True):\n",
    "    message_set = []\n",
    "    for i in range(len(spamham)):\n",
    "        temp = re.sub('[^\\w]',' ',spamham[\"Message\"][i])\n",
    "        temp = temp.lower()\n",
    "        temp = temp.split()    \n",
    "        sentence = \" \"\n",
    "        for word in temp:        \n",
    "            if word not in stopwords.words(\"english\"):\n",
    "                if stem:\n",
    "                    word = stemmer.stem(word)\n",
    "                else:\n",
    "                    word = lemma.lemmatize(word,pos = 'v')\n",
    "                sentence = sentence + word + ' '\n",
    "        message_set.append(sentence)\n",
    "    return (message_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' go jurong point crazi avail bugi n great world la e buffet cine got amor wat ',\n",
       " ' ok lar joke wif u oni ',\n",
       " ' free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri question std txt rate c appli 08452810075over18 ',\n",
       " ' u dun say earli hor u c alreadi say ',\n",
       " ' nah think goe usf live around though ']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create clean message set by running the preprocessing function, this function would remove stopwords and change the words\n",
    "# to their stem form\n",
    "message_set = preprocess(spamham,stem = True)\n",
    "message_set[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Bag of Words \n",
    "\n",
    "After performing all the preprocessing steps,such as removal of stop words, stemming and lemmatization, the next thing that we need to do is to change the text to a tabular form. This can be done using the bag-of-words representation, also called a bag-of-words model, where each row of the table represents each document(message in this case), and the columns represent the words of the text.\n",
    "\n",
    "Bag of Words can be created using CountVectorizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words model\n",
    "cv = CountVectorizer(max_features=4000) # max feature here represents the no. of features we want to select to create the model\n",
    "X = cv.fit_transform(message_set).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the target variable to \"y\"\n",
    "y = spamham['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Model Building & Evaluation - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model using Naive bayes classifier\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9880382775119617"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the accuracy of the model\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1443,    8],\n",
       "       [  12,  209]], dtype=int64)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "confusion = confusion_matrix (y_test, y_pred)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Model Building & Evaluation - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' go jurong point crazy available bugis n great world la e buffet cine get amore wat ',\n",
       " ' ok lar joke wif u oni ',\n",
       " ' free entry 2 wkly comp win fa cup final tkts 21st may 2005 text fa 87121 receive entry question std txt rate c apply 08452810075over18 ',\n",
       " ' u dun say early hor u c already say ',\n",
       " ' nah think go usf live around though ']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create clean message set by running the preprocessing function, this function would remove stopwords and change the words\n",
    "# to their lemma form\n",
    "message_set = preprocess(spamham,stem = False)\n",
    "message_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words model\n",
    "cv = CountVectorizer(max_features=4000)\n",
    "X = cv.fit_transform(message_set).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model using Naive bayes classifier\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9886363636363636"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the accuracy of the model\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1445,    6],\n",
       "       [  13,  208]], dtype=int64)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "confusion = confusion_matrix (y_test, y_pred)\n",
    "confusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
